#stl_error table records internal processing errors generated by the Amazon Redshift database engine. The information in STL_ERROR is useful for troubleshooting certain errors.   */
select * from stl_error where recordtime >= (select starttime from stl_query where query = 6193315 );    

#this is to see the complete error message because sometime the error displayed might be truncated due to length restriction*/
select * from svl_s3log where query = 8219052; 

#Returns execution information about a database query (e.g. actual query text & whether the query was aborted)*/
select * from stl_query where query = 1373;
select * from stl_query where starttime >= '2019-07-22 00:00';
select * from SVL_S3QUERY where query= 730263 order by segment, step, slice;
select * from SVL_S3QUERY_SUMMARY where query= 203032 order by segment, stepv
select * from svl_s3retries where query = 203032;

select * from STL_LOAD_ERRORS where query = 1373;
#  The below queries were used to identify COPY commands stats related to execution times and last file loaded 

select count(1) as copy_count, avg(datediff('s', starttime, endtime)) avg_exec_time, max(datediff('s', starttime, endtime)) max_exec_time, min(datediff('s', starttime, endtime)) min_exec_time, median(datediff('s', starttime, endtime)) med_exec_time from stl_query where <remaining clause without aborted condition>;         

select max(filename) from stl_load_commits where query = (select max(query) max_query from stl_query where querytxt ilike 'copy%bgb_syndication_v2_eu%' and aborted=0);


select * from SVL_QUERY_REPORT where query = 1373 order by 4,5,3; (sort by segment, step, slice)
select * from SVL_QUERY_METRICS_SUMMARY where query = 11908841;
select * from svl_query_summary where query = 11908841 order by query, seg, step;

select * from Stl_wlm_query where service_class > 4 and query = 

select query, trim(querytxt) as sqlquery
from stl_query
where query = 1429pg_last_query_id();

select query, substring(path,0,40) as path
from stl_unload_log
where query=YOURQUERYID
order by path;

#Troubleshoot potential dead locks

STL_TR_CONFLICT
SVV_TRANSACTIONS
STV_LOCKS

/*https://github.com/awslabs/amazon-redshift-utils/blob/master/src/AdminViews/v_check_transaction_locks.sql*/

SELECT 
  current_time, 
  c.relname, 
  l.database, 
  l.transaction, 
  l.pid, 
  a.usename, 
  l.mode, 
  l.granted
FROM pg_locks l 
JOIN pg_catalog.pg_class c ON c.oid = l.relation
JOIN pg_catalog.pg_stat_activity a ON a.procpid = l.pid
WHERE l.pid <> pg_backend_pid();

select distinct(tablename) from pg_table_def where schemaname = 'public'; 

SELECT * FROM PG_TABLE_DEF
WHERE schemaname = 'public' and tablename = '';

#trouble shoot diskspace & lock related issues
#resturns general information of the execution of the query
select * from svl_query_summary where query = MyQueryID order by stm, seg, step;
select * from svl_query_report where query = MyQueryID order by segment, step, elapsed_time, rows;
select * from STL_QUERY_METRICS where query = MyQueryID;
#whether query step spilled the disk
select query, step, rows, workmem, label, is_diskbased
from svl_query_summary
where query = 1025
order by workmem desc;

#WLM Parameter Groups:
aws redshift modify-cluster-parameter-group --parameter-group-name test --parameters "{\"ParameterName\": \"wlm_json_configuration\",\"ParameterValue\": \"[{\\\"query_concurrency\\\": 4,\\\"query_group\\\": [],\\\"query_group_wild_card\\\": 0,\\\"user_group\\\": [],\\\"user_group_wild_card\\\": 0},{\\\"short_query_queue\\\": true}]\",\"ApplyType\":\"dynamic\"}"

select query, pid, elapsed, substring from svl_qlog
order by starttime desc
limit 5;



#Query performance, Partition related issue

#You can use SVL_S3QUERY_SUMMARY to gain insight into several interesting S3 metrics for the query that uses partitioned Parquet files:
#Pay special attention to two interesting metrics:  s3_scanned_rows and s3query_returned_rows.  You would notice the tremendous reduction in the amount of data that returns from Amazon Redshift Spectrum to Amazon Redshift native for the final processing when compared to CSV files.
select * from SVL_S3QUERY_SUMMARY where query=<Query-ID>;


#You can use the following SQL to analyze the effectiveness of partition pruning. If the query touches only a few partitions, you can verify if everything behaves as expected:
SELECT query,
	segment,
	max(assigned_partitions) as total_partitions,
	max(qualified_partitions) as qualified_partitions 
FROM svl_s3partition 
WHERE query=<Query-ID>
GROUP BY 1,2;
#Show which tables are partitioned
select * from SVV_EXTERNAL_PARTITIONS;



aws redshift modify-cluster-parameter-group --parameter-group-name test --parameters "{\"ParameterName\": \"wlm_json_configuration\",\"ParameterValue\": \"[{\\\"query_concurrency\\\": 5,\\\"query_group\\\": [],\\\"query_group_wild_card\\\": 0,\\\"user_group\\\": [],\\\"user_group_wild_card\\\": 0},{\\\"short_query_queue\\\": true}]\",\"ApplyType\":\"dynamic\"}"

#Get more details about stl_load_error: show the filename with errors, incorrect line number, and error details.
select le.starttime, d.query, d.line_number, d.colname, d.value, le.raw_line, le.err_reason    
from stl_loaderror_detail d, stl_load_errors le
where d.query = 450294
and le.tbl = 182679
order by le.starttime desc
limit 3;

select pg_terminate_backend(26282)

#long running transactions that cause node to reboot
/*The crash was caused by an out of memory error on the leader node, this oom was due to very long running idle transactions that were left open by some user session. 
Unfortunately, since the cluster was rebooted we do not know which user left their sessions open. 
This is usually common where users open a transaction with BEGIN and never close the transaction with COMMIT, END or ROLLBACK. 
These long running transactions would have caused memory pressure on the leader node as the database has to keep track of all changes made to the database since the long running transaction started. 
Over time, most of the memory on the leader was used up and this caused the UNDO process on xid 9577709 to not have enough memory to execute. 
Since UNDO is a critical process, the cluster crashed in order to recover. We have fixes for these issues coming in future patches. 
In order to mitigate this, the customer needs to monitor the number of long running inactive transactions that are on the database and kill such sessions if they remain open and inactive for too long. 
Customer can use the query below to help identify these sessions.*/

select 
  *,
  datediff(s,txn_start,getdate())/86400||' days '||datediff(s,txn_start,getdate())%86400/3600||' hrs '||datediff(s,txn_start,getdate())%3600/60||' mins '||datediff(s,txn_start,getdate())%60||' secs' 
from svv_transactions 
where 
  lockable_object_type='transactionid' and  pid<>pg_backend_pid() 
order by 3;



select 
  trim(pu.usename) as username, trim(sti.schema) as schema, trim(sti.table) as table,  sq.query as query_id, trim(sq.querytxt) as text
from 
  (select 
    distinct userid, query, tbl 
  from stl_scan) ss 
left join SVV_TABLE_INFO sti on sti.table_id = ss.tbl 
left join pg_user pu on pu.usesysid = ss.userid
left join stl_query sq on ss.query = sq.query


#monitor long running transacations and send alerts:

#There isn't an automated way to do this. Customer can create the view below and use a script to log in at half hourly intervals to check for transactions that have been idle for more than 30 mins. 
#The idle_secs column would display how long a transaction has been idle for, customer can use this value to trigger an SNS notification that will alert them to transactions that have been idle for more than 30 mins (1800s). 

create or replace view v_open_transaction_state as 
select trim(a.txn_owner) as txn_owner,trim(a.txn_db) as txn_db,a.xid,a.pid,a.txn_start,getdate() as currenttime,datediff(s,a.txn_start,getdate())/86400||' days '||datediff(s,a.txn_start,getdate())%86400/3600||' hrs '||datediff(s,a.txn_start,getdate())%3600/60||' mins '||case when a.txn_start > getdate() then 0 else datediff(s,a.txn_start,getdate())%60 end||' secs' as txn_duration,c.query as prev_query_id,c.prev_query_endtime,c."type" as prev_query_type,c.prev_querytext,b.query as current_query_id,b.starttime as current_query_starttime,case when b.starttime is not null and getdate() > b.starttime then datediff(s,b.starttime,getdate()) when b.starttime is not null then 0 else null end as secs_since_current_query,trim(substring(b.text,1,75)) as current_querytext,case when datediff(s,c.prev_query_endtime,getdate()) >= 60 and b.starttime is null then datediff(s,c.prev_query_endtime,getdate()) else null end as idle_secs,case when datediff(s,c.prev_query_endtime,getdate()) < 60 or b.starttime is not null then 'ACTIVE' else 'IDLE' end as transaction_state 
from svv_transactions a 
left join stv_inflight b on a.xid=b.xid 
left join (select x.* from (select stmt.xid,stmt.query_id as query,stmt.endtime as prev_query_endtime,stmt."type",substring(stmt.text,1,75) as prev_querytext,row_number() over (partition by stmt.xid order by stmt.endtime desc) as rownum from ((SELECT stl_ddltext.userid, stl_ddltext.xid, stl_ddltext.pid, trim(stl_ddltext."label") as "label", stl_ddltext.starttime, stl_ddltext.endtime, stl_ddltext."sequence", ('DDL'::character varying)::character varying(10) AS "type",null as query_id, null::int as aborted, stl_ddltext.text FROM stl_ddltext 
UNION ALL 
SELECT stl_utilitytext.userid, stl_utilitytext.xid, stl_utilitytext.pid, trim(stl_utilitytext."label") as "label", stl_utilitytext.starttime, stl_utilitytext.endtime, stl_utilitytext."sequence", ('UTILITY'::character varying)::character varying(10) AS "type",null as query_id, null::int as aborted, stl_utilitytext.text FROM stl_utilitytext) 
UNION ALL 
SELECT stl_query.userid, stl_query.xid, stl_query.pid, trim(stl_query."label") as "label", stl_query.starttime, stl_query.endtime, stl_querytext."sequence", ('QUERY'::character varying)::character varying(10) AS "type",stl_querytext.query as query_id, stl_query.aborted as aborted, stl_querytext.text 
FROM stl_query, stl_querytext WHERE stl_query.query = stl_querytext.query) stmt where stmt.sequence=0 order by stmt.xid,stmt.endtime desc) x where x.rownum=1) c on a.xid=c.xid 
where a.lockable_object_type='transactionid' 
and a.pid<>pg_backend_pid() 
order by a.xid;

Query to check blocking PIDs - https://github.com/awslabs/amazon-redshift-utils/blob/master/src/AdminViews/v_check_transaction_locks.sql

-- long open sessions
select trim(event) event,recordtime,trim(remotehost)
remotehost,trim(remoteport) remoteport,pid,trim(dbname)
dbname,trim(username) username,trim(authmethod)
authmethod,trim(sslversion) ssl,trim(sslcipher)
sslcipher,mtu,duration,(duration/1000000/3600)||'hrs '||(duration/1000000%3600)/60||'mins '||(duration/1000000%60)||'secs'  from stl_connection_log  where duration and username<>'rdsdb' order by duration desc limit 100;

-- long open transaction (only gives information for currently held locks) 
select *,datediff(s,txn_start,getdate())/86400||' days '||datediff(s,txn_start,getdate())%86400/3600||' hrs '||datediff(s,txn_start,getdate())%3600/60||' mins '||datediff(s,txn_start,getdate())%60||' secs' from svv_transactions where lockable_object_type='transactionid' and pid<>pg_backend_pid() order by 3;

-- find all statements in pid (first 200 characters of each statement)
select * from svl_statementtext where pid=PID and sequence=0 order by starttime;


select * from svv_table_info;
select * from svv_table_info where schema='public' and table='temp_ctl_proc_step';
select len(nspname) from pg_namespace;

#find out which query / COPY command was run against which table

select *
from stl_load_commits
where filename like '%dim_pn_alert%' order by query;


select 
  trim(pu.usename) as username, trim(sti.schema) as schema, trim(sti.table) as table,  sq.query as query_id, trim(sq.querytxt) as text
from 
  (select 
    distinct userid, query, tbl 
  from stl_scan) ss 
left join SVV_TABLE_INFO sti on sti.table_id = ss.tbl 
left join pg_user pu on pu.usesysid = ss.userid
left join stl_query sq on ss.query = sq.query
where sq.starttime >= '2019-04-10 00:00' and sq.endtime < '2019-04-11 00:00';



select userid, xid,  pid, query, segment, locus,  
datediff(ms, starttime, endtime) as duration, compile 
from svl_compile 
where query = 3816511
order by query, segment;



SELECT customer_order_item_id, listagg(customer_shipment_item_id::text, ',') FROM BOOKER.D_UNIFIED_

SELECT customer_order_item_id, listagg(customer_shipment_item_id, ',') FROM BOOKER.D_UNIFIED_CUST_S




select * from SVL_QUERY_REPORT where query = 3804836 order by 4,5,3;
select * from SVL_QUERY_REPORT where query = 3816511 order by 4,5,3;
select * from SVL_QUERY_REPORT where query = 3827990 order by 4,5,3;



select * from SVL_QUERY_METRICS_SUMMARY where query = 3804836;
select * from SVL_QUERY_METRICS_SUMMARY where query = 3816511;
select * from SVL_QUERY_METRICS_SUMMARY where query = 3827990;

SELECT
  customer_order_item_id, count(*)
FROM
  BOOKER.D_UNIFIED_CUST_SHIPMENT_ITEMS
WHERE
  order_day BETWEEN (TO_DATE( '20190401', 'YYYYMMDD' ) - 14) and TO_DATE( '20190401', 'YYYYMMDD' )
GROUP BY
  1
limit 10;


SELECT 
  customer_order_item_id, count(*) 
FROM 
  BOOKER.D_UNIFIED_CUST_SHIPMENT_ITEMS 
WHERE 
  order_day BETWEEN (TO_DATE( '20190401', 'YYYYMMDD' ) - 14) and TO_DATE( '20190401', 'YYYYMMDD' ) 
GROUP BY 
  1 
limit 10;



#Show all locks 

select
 a.database,
 a.relation,
 a.pid,
 a.mode,
 a.granted,
 s.*
from pg_locks a
left join stv_tbl_perm s on a.relation=s.id;

select * from stl_tr_conflict where table_id=100234
order by xact_start_ts;




SELECT svl_s3retries.query, svl_s3retries.segment, svl_s3retries.node, svl_s3retries.slice, svl_s3retries.eventtime, svl_s3retries.retries, 
svl_s3retries.successful_fetches, svl_s3retries.file_size, btrim((svl_s3retries."location")::text) AS "location", btrim((svl_s3retries.message)::text)
AS message FROM svl_s3retrie where svl_s3retries.query = 8219052;


aws redshift modify-cluster-maintenance --region us-east-1 --cluster-identifier miaojoe-work  --defer-maintenance --defer-maintenance-start-time 2019-04-23T12:00:00Z --defer-maintenance-duration 5
aws redshift describe-clusters --region us-east-1 --cluster-identifier miaojoe-work --query 'Clusters[*].DeferredMaintenanceWindows[]'
aws redshift modify-cluster-maintenance --region us-east-1 --cluster-identifier miaojoe-work  --no-defer-maintenance --defer-maintenance-identifier dfm-aaExPfMz1fmGnyK0vkAB

#Check underreped blocks
select sysdate, node,count(*) from stv_underrepped_blocks join stv_slices using (slice) group by 2 order by 2;

# identify opportunities to improve query performance.
https://docs.aws.amazon.com/redshift/latest/dg/r_STL_ALERT_EVENT_LOG.html


#tables with missing statistics
SELECT database, schema || '.' || "table" AS "table", stats_off 
FROM svv_table_info 
WHERE stats_off > 5 
ORDER BY 2;

https://api.advisor.redshift.aws.a2z.com/

select date_trunc('d', recordtime) "hour",  trim(username) username, count(1) num_connections  from stl_connection_log where event='authenticated' and username <> 'rdsdb' group by 1, 2 order by 1,2;


#queries that use the most resources (long running queries):
SELECT trim(DATABASE)           AS DB,
  COUNT(query)                  AS n_qry,
  MAX(substring (qrytext,1,80)) AS qrytext,
  MIN(run_seconds)              AS "min" , --seconds
  MAX(run_seconds)              AS "max",
  AVG(run_seconds)              AS "avg",
  SUM(run_seconds)              AS total,
  MAX(query)                    AS max_query_id,
  MAX(starttime)::DATE          AS last_run,
  aborted,
  event
FROM
  (SELECT userid,
    label,
    stl_query.query,
    trim(DATABASE)      AS DATABASE,
    trim(querytxt)      AS qrytext,
    md5(trim(querytxt)) AS qry_md5,
    starttime,
    endtime,
    DATEDIFF(seconds, starttime,endtime)::NUMERIC(12,2) AS run_seconds,
    aborted,
    DECODE(alrt.event,'Very selective query filter','Filter','Scanned a large number of deleted rows','Deleted','Nested Loop Join in the query plan','Nested Loop','Distributed a large number of rows across the network','Distributed','Broadcasted a large number of rows across the network','Broadcast','Missing query planner statistics','Stats',alrt.event) AS event
  FROM stl_query
  LEFT OUTER JOIN
    (SELECT query,
      trim(split_part(event,':',1)) AS event
    FROM STL_ALERT_EVENT_LOG
    WHERE event_time >= DATEADD(DAY, -7, CURRENT_DATE)
    GROUP BY query,
      trim(split_part(event,':',1))
    ) AS alrt
  ON alrt.query = stl_query.query
  WHERE userid  <>1
    -- and (querytxt like 'SELECT%' or querytxt like 'select%' )
    -- and database = ''
  AND starttime >= DATEADD(DAY, -7, CURRENT_DATE)
  )
GROUP BY DATABASE,
  label,
  qry_md5,
  aborted,
  event
ORDER BY total DESC limit 50;


#WLM + Commit queuing (which can show high concurrency activity):
SELECT IQ.*,
  ((IQ.wlm_queue_time::FLOAT   /IQ.wlm_start_commit_time)*100)::DECIMAL(5,2) AS pct_wlm_queue_time,
  ((IQ.exec_only_time::FLOAT   /IQ.wlm_start_commit_time)*100)::DECIMAL(5,2) AS pct_exec_only_time,
  ((IQ.commit_queue_time::FLOAT/IQ.wlm_start_commit_time)*100)::DECIMAL(5,2) pct_commit_queue_time,
  ((IQ.commit_time::FLOAT      /IQ.wlm_start_commit_time)*100)::DECIMAL(5,2) pct_commit_time
FROM
  (SELECT TRUNC(b.starttime) AS DAY,
    d.service_class,
    c.node,
    COUNT(DISTINCT c.xid)                                            AS count_commit_xid,
    SUM(DATEDIFF('microsec', d.service_class_start_time, c.endtime)) AS wlm_start_commit_time,
    SUM(DATEDIFF('microsec', d.queue_start_time, d.queue_end_time )) AS wlm_queue_time,
    SUM(DATEDIFF('microsec', b.starttime, b.endtime))                AS exec_only_time,
    SUM(DATEDIFF('microsec', c.startwork, c.endtime)) commit_time,
    SUM(DATEDIFF('microsec', DECODE(c.startqueue,'2000-01-01 00:00:00',c.startwork,c.startqueue), c.startwork)) commit_queue_time
  FROM stl_query b ,
    stl_commit_stats c,
    stl_wlm_query d
  WHERE b.xid         = c.xid
  AND b.query         = d.query
  AND c.xid           > 0
  AND d.service_class > 4
  GROUP BY TRUNC(b.starttime),
    d.service_class,
    c.node
  ORDER BY TRUNC(b.starttime),
    d.service_class,
    c.node
  ) IQ; 
  

#STL_ALERT_EVENT_LOG table to identify opportunities to improve query performance.

SELECT query, substring(event,0,25) as event, 
substring(solution,0,25) as solution, 
trim(event_time) as event_time from stl_alert_event_log order by query;



#queries that spilled disk
SELECT q.query, trim(q.cat_text)
FROM (
  SELECT query, 
    replace( listagg(text,' ') WITHIN GROUP (ORDER BY sequence), '\\n', ' ') AS cat_text 
    FROM stl_querytext 
    WHERE userid>1 
    GROUP BY query) q
JOIN (
  SELECT distinct query 
  FROM svl_query_summary 
  WHERE is_diskbased='t' 
  AND (LABEL LIKE 'hash%' OR LABEL LIKE 'sort%' OR LABEL LIKE 'aggr%') 
  AND userid > 1) qs 
ON qs.query = q.query;

--Return Alerts from past 7 days
https://github.com/awslabs/amazon-redshift-utils/blob/master/src/AdminScripts/perf_alert.sql

--tables with large VARCHAR columns:
SELECT database, schema || '.' || "table" AS "table", max_varchar 
FROM svv_table_info 
WHERE max_varchar > 150 
ORDER BY 2;

--determin the true width
SELECT max(len(rtrim(column_name))) 
#If you find that the table has columns that are wider than necessary, then you need to re-create a version of the table with appropriate column widths by performing a deep copy.
FROM table_name;


https://github.com/awslabs/amazon-redshift-utils/blob/master/src/AdminScripts/table_inspector.sql


--If you find that you have tables with skewed distribution keys, then consider changing the distribution key to a column that exhibits high cardinality and uniform distribution. Evaluate a candidate column as a distribution key by creating a new table using CTAS:
CREATE TABLE my_test_table DISTKEY (<column name>) AS SELECT <column name> FROM <table name>;

--Don't apply SQL function to a sortkey


--To determine whether a query used the result cache, query the SVL_QLOG system view. 
--If a query used the result cache, the source_query column returns the query ID of the source query. If result caching wasn't used, the source_query column value is NULL.

select userid, query, elapsed, source_query from svl_qlog 
where userid > 1
order by query desc;

userid | query  | elapsed  | source_query
-------+--------+----------+-------------
   104 | 629035 |       27 |       628919


--Vacuum
-- 1. "STV_TBL_PERM" table contains two columns "rows" and "sorted_rows", which represent the total amount and total sorted amount of rows per slice; if two values do not match - VACUUM is advised;
https://docs.aws.amazon.com/redshift/latest/dg/r_STV_TBL_PERM.html
-- 2. "SVV_TABLE_INFO" view contains "unsorted" column, which represents the percentage of unsorted rows in the table;

#WLM config change failed errors:

select userid,recordtime,pid,trim(error_string) as error_string from stl_wlm_error order by recordtime desc limit 10;

--If you run this query you will see that because it failed before the cluster has applied the default parameters
select stvc.service_class,trim(stvw.condition) as condition,trim(stvc.name) name,stvc.num_query_tasks,stvc.target_num_query_tasks,user_group_wild_card,query_group_wild_card,stvc.evictable,stvc.eviction_threshold,stvc.query_working_mem,stvc.target_query_working_mem,stvc.max_execution_time 
from STV_WLM_SERVICE_CLASS_CONFIG stvc left join 
(select action_service_class,listagg(condition, ' ') within group (order by condition desc) as condition 
  from STV_WLM_CLASSIFICATION_CONFIG group by action_service_class) stvw 
on stvc.service_class=stvw.action_service_class 
order by stvc.service_class;



#check privilages of users/usergoups on schemas/tables 

SELECT
    g.groname,
    u.usename,
    s.schemaname,
    s.tablename,
    has_schema_privilege(u.usename,s.schemaname,'create') AS user_has_create_permission,
    has_schema_privilege(u.usename,s.schemaname,'usage') AS user_has_usage_permission,
    has_table_privilege(u.usename,s.tablename,'select') AS user_has_select_permission,
    has_table_privilege(u.usename,s.tablename,'insert') AS user_has_insert_permission,
    has_table_privilege(u.usename,s.tablename,'delete') AS user_has_delete_permission
FROM
    pg_user u, pg_group g
CROSS JOIN
    (SELECT DISTINCT schemaname, tablename FROM pg_tables) s
where u.usesysid = ANY(g.grolist) and 
g.groname='ro_group'
and s.schemaname='public';



#list all schemas
select nspname
from pg_catalog.pg_namespace;


#Track conflict on table
select * from STL_TR_CONFLICT where abort_time like '2019-07-22 01%';


select * from stl_query 
where starttime like '2019-07-22 01%' and database='eu_mdd_sales_dwh' 
and querytxt like '%SEC_SA_MAPPING_SFE%;

#Redshift events
aws redshift describe-events --start-time 2019-07-28T22:00Z --source-type cluster --source-identifier itx-ags-prd-rs-cl-01


#Estimate Resize Time (https://docs.aws.amazon.com/redshift/latest/mgmt/rs-resize-tutorial.html#rs-tutorial-using-the-resize-operation)
Total data to transfer = Storage for the node type (0.16TB or 160GB for dc2.large) x % disk space used before resize x # of nodes
For dc1/dc2 and ds1/ds2 large cluster, the transfer rate is 20 MB per second per node
For 8xlarge cluster, the transfer rate is 3 times ~ 100 MB per second per node.

For example 

Data = 2560GB x 100% x 2 = 5120 GB

Estimated Time =  5120 GB / (100MB/s/node x 2 nodes) = 5120GB / 0.2 GB = 5360 seconds ~= 1.49 hours



